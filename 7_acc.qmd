# 7 Accountability (ACC)

The requirement of accountability complements the above requirements, and is closely linked to the principle of fairness. It necessitates that mechanisms be put in place to ensure responsibility and accountability for AI systems and their outcomes, both before and after their development, deployment and use.

## 7.1 Auditability

Auditability entails the enablement of the assessment of algorithms, data and design processes. This does not necessarily imply that information about business models and intellectual property related to the AI system must always be openly available. Evaluation by internal and external auditors, and the availability of such evaluation reports, can contribute to the trustworthiness of the technology. In applications affecting fundamental rights, including safety-critical applications, AI systems should be able to be independently audited.

***None of the identified requirements could be mapped to this subcategory. Auditability, however, is a main aspect of the WP1, with audits being performed throughout the project that will, in a co-creation approach, lead to framework updates via interdisciplinary collaboration.***

## 7.2 Minimisation and reporting of negative impacts

Both the ability to report on actions or decisions that contribute to a certain system outcome, and to respond to the consequences of such an outcome, must be ensured. Identifying, assessing, reporting and minimising the potential negative impacts of AI systems is especially crucial for those (in)directly affected. Due protection must be available for whistle-blowers, NGOs, trade unions or other entities when reporting legitimate concerns about an AI-based system. The use of impact assessments (e.g. red teaming or forms of Algorithmic Impact Assessment) both prior to and during the development, deployment and use of AI systems can be helpful to minimise negative impact. These assessments must be proportionate to the risk that the AI systems pose.

### Requirement 72-1-ACC {#72-1-acc}

**Description:** Algorithmic impact assessment following ***\[relevant method\]*** is performed.

| **Parameter**        | **Values**                                  |
|----------------------|---------------------------------------------|
| **Relevant methods** | \- MDR<br>- EU AI Act<br>- To be determined |

<div style="background-color:#f9e79f; padding:10px; border-radius:5px;">
**Development**
<br>**Testing*
<br>**Validation**
</div>

**Owner**

-   WP 1 lead

::: {style="background-color:#e6f7ff; padding:10px; border-radius:5px;"}
**To Be Done**
<br>**Note**: algorithmic impact assessment will be determined by the EU AI act.
:::

## 7.3 Trade-offs

When implementing the above requirements, tensions may arise between them, which may lead to inevitable trade-offs. Such trade-offs should be addressed in a rational and methodological manner within the state of the art. This entails that relevant interests and values implicated by the AI system should be identified and that, if conflict arises, trade-offs should be explicitly acknowledged and evaluated in terms of their risk to ethical principles, including fundamental rights. In situations in which no ethically acceptable trade-offs can be identified, the development, deployment and use of the AI system should not proceed in that form. Any decision about which trade-off to make should be reasoned and properly documented. The decision-maker must be accountable for the manner in which the appropriate trade-off is being made, and should continually review the appropriateness of the resulting decision to ensure that necessary changes can be made to the system where needed.

### Requirement 73-1-ACC {#73-1-acc}

**Description:** Research work produced by the VALIDATE consortium must adhere to ***\[guidelines\]*** as well as ethical norms regarding ***\[relevant aspects\]***.

| **Parameter**        | **Values**                                                                                                         |
|---------------------------------------|---------------------------------|
| **Guidelines**       | \- EU Horizon funding guidelines<br>- Reporting guidelines, e.g., TRIPOD<br>                                       |
| **Relevant methods** | \- Data availability<br>- Reproducibility<br>- Open access<br>- Distribution<br>- Confidentiality<br>- Originality |

<div style="background-color:#f9e79f; padding:10px; border-radius:5px;">
**Development**
<br>**Testing**
<br>**Validation**
</div>

**Owner**

-   WP 1 lead

::: {style="background-color:#e6f7ff; padding:10px; border-radius:5px;"}
**Fulfilled**
<br>**See Artefacts:**
<a href=“https://charitede.sharepoint.com/:w:/r/sites/Teammad.AI-Projects_VALIDATE_general/Shared%20Documents/Projects_VALIDATE_general/Open%20Research%20Norms.docx?d=wfd86b969ca634cd4bc842e56f8bec3b3&csf=1&web=1&e=bEHsKf”> Open Research Norms Charter</a>
:::

## 7.4 Redress

When unjust adverse impact occurs, accessible mechanisms should be foreseen that ensure adequate redress. Knowing that redress is possible when things go wrong is key to ensure trust. Particular attention should be paid to vulnerable persons or groups.

***None of the identified requirements could be mapped to this subcategory.***
