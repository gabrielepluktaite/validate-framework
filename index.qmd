---
title: "Introduction to VALIDATE Framework"
author: ""
date: ""
output:
  html_document:
    toc: true
  pdf_document:
    toc: true
---

## Overview

Trustworthy AI in healthcare is crucial to ensure positive patient outcomes, especially in high-stakes areas like stroke prognosis. 

This website presents the **VALIDATE Trustworthy AI Framework**, a resource we have developed to support the ethical development of an AI-based prognostic tool for outcome prediction of acute stroke patients. See [here](https://validate-project.eu) for more on the VALIDATE project. 

## Purpose
AI tools in healthcare must be trustworthy and reliable. To achieve this, they must be developed in compliance with ethical and legal standards that prevent biases, ensure patient safety, and maintain transparency. 

We have designed the VALIDATE Trustworthy AI Framework to help researchers, clinicians, and AI developers within the VALIDATE research consortium operationalize high-level ethical principles into practical applications. We aim to ensure that the AI tools developed meet stringent standards in terms of regulation, legality, ethics, and robustness---aligning with the [EU Guidelines for Trustworthy AI](https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai).

## Methodology
Although several principle-based ethical guidelines for the development of trustworthy AI exist, their application can be hindered by their high level of abstraction. Most ethics frameworks provide recommendations on *what* to achieve, but offer limited guidance on *how* to implement these principles in practice. Recognizing this gap, we have developed a comprehensive methodological toolbox for the VALIDATE project to bridge high-level guidelines with practical requirements.

Our approach includes:

- **Co-creation of Low-Level Requirements**: Through workshops with a variety of project stakeholders, we identified project-specific challenges and aligned them with high-level policy recommendations, producing actionable low-level requirements.

- **Z-Inspection Analysis**: We engaged external experts to evaluate the ethical requirements of the VALIDATE system, ensuring independent oversight and alignment with best practices.

- **Periodic Audits**: We conduct regular audits with consortium members to ensure compliance with evolving ethical standards. These audits involve reviewing and updating requirements and their features based on stakeholder feedback and reflection. 

Overall, the tools we have developed are embedded in an interdisciplinary, co-creation approach, resulting in a dynamic framework that evolves as new ethical, legal, and technological insights emerge.

## Version-controlled, Dynamic Development
Our ethical framework functions as a living document and is systematically updated at key milestones (9, 18, and 36 project months) to reflect new findings and discussions. Version control ensures transparency and accountability in the development process, ensuring that all stakeholders are working with the most current and accurate information, fostering ongoing alignment with evolving ethical and legal standards. The website shown here reflects the current snapshot of the framework, while previous (and future) versions of the document will be archived in the project's [git repository](https://github.com/gabrielepluktaite/validate-framework). Any changes between audits will similarly be recorded and thus available in the files' version histories.

## Why this Matters
By designing and implementing the **VALIDATE Trustworthy AI Framework**, we aim to create an ethical, reliable, and transparent AI solution for clinical support. Our procedure and framework serve as a pivotal example for the safe and transparent development of AI-based clinical decision-support systems, setting a benchmark for future projects in healthcare AI. 

We invite you to explore the framework, provide feedback, and engage with our ongoing efforts to advance trustworthy AI in clinical settings.
