[
  {
    "objectID": "0_organization.html",
    "href": "0_organization.html",
    "title": "How Requirements are Organized",
    "section": "",
    "text": "Requirement YZ-N-AAA Example\nEach requirement has a unique ID, constructed as follows:\nYZ: Sub-chapter numbers reflecting the requirement’s area\nN: Sequential number within the sub-chapter\nAAA: The first three letters of the chapter topic\nDescription: X of [example parameter] for measuring model uncertainty are implemented to provide a confidence interval for predictions in the app available at [time point].",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>How Requirements are Organized</span>"
    ]
  },
  {
    "objectID": "0_organization.html#requirement-yz-n-aaa-example",
    "href": "0_organization.html#requirement-yz-n-aaa-example",
    "title": "How Requirements are Organized",
    "section": "",
    "text": "Parameters\n\n\n\n\n\n\n\nParameter\nValues\n\n\n\n\nExample Parameter\nParameters listed in brackets are adjustable by the WP 1 team during the project period.\n\n\nTime Point\nRefers to the deadline by which a requirement should be met, typically specifying some due date.\n\n\n\n\n\nPhases\nEach requirement is mapped to a specific project phase:\nDevelopment: Focuses on developing the tool’s prototype until month 18.\nTesting: The prototype undergoes testing until month 18.\nValidation: Begins in month 18, marking the start of validating the developed and tested prototype.\n\n\nStakeholders and Owner\nStakeholders: Includes project staff and other relevant groups affected by or necessary for fulfilling the requirement.\nOwner: Each requirement is assigned an Owner, usually a WP lead relevant to existing tasks. The Owner is responsible for task delegation and execution to meet the requirement.\n\n\nRequirement Categorization\nBased on the parameterized statements, requirements are classified into three sub-categories:\n\nTolerable: The minimum achievement threshold for a requirement.\n\n\nGoal: Represents the objective aimed to be reached by the project. If both Tolerable and Goal are defined, the project’s likely outcome will be between these two.\n\n\nWish: Desirable outcomes that are aimed for but likely only partially achieved.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>How Requirements are Organized</span>"
    ]
  },
  {
    "objectID": "1_hum.html",
    "href": "1_hum.html",
    "title": "1 Human Agency and Oversight (HUM)",
    "section": "",
    "text": "1.1 Fundamental rights\nLike many technologies, AI systems can equally enable and hamper fundamental rights. They can benefit people for instance by helping them track their personal data, or by increasing the accessibility of education, hence supporting their right to education. However, given the reach and capacity of AI systems, they can also negatively affect fundamental rights. In situations where such risks exist, a fundamental rights impact assessment should be undertaken. This should be done prior to the system’s development and include an evaluation of whether those risks can be reduced or justified as necessary in a democratic society in order to respect the rights and freedoms of others. Moreover, mechanisms should be put into place to receive external feedback regarding AI systems that potentially infringe on fundamental rights.\nNone of the identified requirements could be mapped to this category. Other than the requirements addressed in related sections such as privacy, we believe that our tool does not influence fundamental rights.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1 Human Agency and Oversight (HUM)</span>"
    ]
  },
  {
    "objectID": "1_hum.html#human-agency",
    "href": "1_hum.html#human-agency",
    "title": "1 Human Agency and Oversight (HUM)",
    "section": "1.2 Human Agency",
    "text": "1.2 Human Agency\nUsers should be able to make informed autonomous decisions regarding AI systems. They should be given the knowledge and tools to comprehend and interact with AI systems to a satisfactory degree and, where possible, be enabled to reasonably self-assess or challenge the system. AI systems should support individuals in making better, more informed choices in accordance with their goals. AI systems can sometimes be deployed to shape and influence human behaviour through mechanisms that may be difficult to detect, since they may harness sub-conscious processes, including various forms of unfair manipulation, deception, herding and conditioning, all of which may threaten individual autonomy. The overall principle of user autonomy must be central to the system’s functionality. Key to this is the right not to be subject to a decision based solely on automated processing when this produces legal effects on users or similarly significantly affects them.\n\nRequirement 12-1-HUM\nDescription: [relevant methods] for measuring model uncertainty are implemented to provide a confidence interval for predictions in the app available at [time point].\n\n\n\nParameter\nValues\n\n\n\n\nRelevant Methods\nTo be defined by Owner\n\n\nTime Point\nBy the start of the prospective study\n\n\n\nPhases: development, testing, validation\nOwner\n\nWP 2 lead\n\nStakeholders\n\nWP 2 (ML)\nWP 3 (design)\nWP 4 (clinical validation)\nOther: medical staff\n\n\nTolerable\n\nAll of the relevant methods: to be defined by Owner for measuring model uncertainty are used to provide a confidence interval for predictions in the app available at start of the prospective study.\n\n\n\n\nRequirement 12-2-HUM\nDescription: In the events where there is a clear conflict between the predictions of the tool and the medical opinion of the doctor, a process protocol for epistemic authority dilemmas customized for [physician expertise level] can be followed. Available by [time point].\n\n\n\n\n\n\n\nParameter\nValues\n\n\n\n\nPhysician Expertise Level\n- Beginner\n- Intermediate\n- Expert\n\n\nTime Point\nBy the end of the project\n\n\n\nPhases: development, testing, validation\nOwner\n\nWP 1 lead\n\nStakeholders\n\nWP 1 (ethics)\nWP 3 (design)\nWP 4 (clinical validation)\nOther: medical staff\n\n\nTolerable\n\nIn the events where there is a clear conflict between the predictions of the tool and the medical opinion of the doctor, a process protocol for epistemic authority dilemmas customized for beginner, intermediate, expert can be followed. Available by project end.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1 Human Agency and Oversight (HUM)</span>"
    ]
  },
  {
    "objectID": "1_hum.html#human-oversight",
    "href": "1_hum.html#human-oversight",
    "title": "1 Human Agency and Oversight (HUM)",
    "section": "1.3 Human Oversight",
    "text": "1.3 Human Oversight\nHuman oversight helps ensuring that an AI system does not undermine human autonomy or causes other adverse effects. Oversight may be achieved through governance mechanisms such as a human-in-the-loop (HITL), human-on-the-loop (HOTL), or human-in-command (HIC) approach.  \nHITL refers to the capability for human intervention in every decision cycle of the system, which in many cases is neither possible nor desirable.  \nHOTL refers to the capability for human intervention during the design cycle of the system and monitoring the system’s operation.  \nHIC refers to the capability to oversee the overall activity of the AI system (including its broader economic, societal, legal and ethical impact) and the ability to decide when and how to use the system in any particular situation. This can include the decision not to use an AI system in a particular situation, to establish levels of human discretion during the use of the system, or to ensure the ability to override a decision made by a system.  \nMoreover, it must be ensured that public enforcers have the ability to exercise oversight in line with their mandate. Oversight mechanisms can be required in varying degrees to support other safety and control measures, depending on the AI system’s application area and potential risk. All other things being equal, the less oversight a human can exercise over an AI system, the more extensive testing and stricter governance is required. \n\nRequirement 13-1-HUM\nDescription: Tool is designed with [capability needed] to enable a Human in Command (HIC) governance structure. This is implemented by [time point].\n\n\n\n\n\n\n\nParameter\nValues\n\n\n\n\nCapability Needed\n- Capability to oversee the overall activity of the AI system- Ability to decide whether, when and how to use the system in any particular situation- Established levels of human discretion during the use of the system where necessary- Ability to override a decision made by a system\n\n\nTime Point\nBy the end of the project\n\n\n\nOwner\n\nWP 1 lead\n\nStakeholders\n\nWP 1 (ethics)\nWP 3 (design)\nWP 4 (clinical validation)\n\n\nTolerable\n\nTool is designed with capability to oversee the overall activity of the AI system to enable a HIC governance structure. This is implemented by the end of the project.\n\n\n\nGoal\n\nTool is designed with ability to decide whether, when and how to use the system in any particular situation to enable a HIC governance structure. This is implemented by the end of the project.\nTool is designed with established levels of human discretion during the use of the system where necessary to enable a HIC governance structure. This is implemented by the end of the project.\nTool is designed with ability to override a decision made by a system to enable a HIC governance structure. This is implemented by the end of the project.\n\n\n- Tool is designed with established levels of human discretion during the use of the system where necessary to enable a HIC governance structure. This is implemented by the end of the project.\n- Tool is designed with ability to override a decision made by a system to enable a HIC governance structure. This is implemented by the end of the project.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1 Human Agency and Oversight (HUM)</span>"
    ]
  },
  {
    "objectID": "2_rob.html",
    "href": "2_rob.html",
    "title": "2 Technical Robustness and Safety (ROB)",
    "section": "",
    "text": "2.1 Resilience to Attack and Safety\nAI systems, like all software systems, should be protected against vulnerabilities that can allow them to be exploited by adversaries, e.g. hacking. Attacks may target the data (data poisoning), the model (model leakage) or the underlying infrastructure, both software and hardware. If an AI system is attacked, e.g. in adversarial attacks, the data as well as system behaviour can be changed, leading the system to make different decisions, or causing it to shut down altogether. Systems and data can also become corrupted by malicious intention or by exposure to unexpected situations. Insufficient security processes can also result in erroneous decisions or even physical harm. For AI systems to be considered secure, possible unintended applications of the AI system (e.g. dual-use applications) and potential abuse of the system by malicious actors should be taken into account, and steps should be taken to prevent and mitigate these.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2 Technical Robustness and Safety (ROB)</span>"
    ]
  },
  {
    "objectID": "2_rob.html#resilience-to-attack-and-safety",
    "href": "2_rob.html#resilience-to-attack-and-safety",
    "title": "2 Technical Robustness and Safety (ROB)",
    "section": "",
    "text": "Requirement 21-1-ROB\nDescription: Tool should follow relevant aspects of norms that are within our scope and resource limitations to prepare for compliance with the MDR by time point.\n\n\n\n\n\n\n\nParameter\nValues\n\n\n\n\nRelevant Aspects of Norms\nISO norms related to the standard MDR:- IEC 62366- IEC 62304- ISO 14971 ISO norms related to MDR and recent AI advancements:- ISO 13485- ISO/IEC 23894:2023- ISO/IEC 38507\n\n\nTime Point\nThe end of the project\n\n\n\nPhases: development, testing, validation\nOwner\n\nWP 5 lead\n\nStakeholders\n\nWP 2 (development)\n\nWP 3 (design)\n\nWP 5 (regulatory pathway to market)\n\n\nWish\n\nThe stability of the accuracies and performance for when there are missing input data points of the tool is analyzed or monitored, available by the start of the validation phase.\nTool should follow relevant aspects of ISO norms related to the MDR and recent AI advancements: ISO 13485; ISO/IEC 23894:2023; ISO/IEC 38507 that are within our scope and resource limitations to prepare for compliance with the MDR by the end of the project.\n\n\n\n\nRequirement 21-2-ROB\nDescription: Tool should follow relevant guidelines on cyber security that are within our scope and resource limitations to prepare for compliance with the MDR by time point.\n\n\n\n\n\n\n\nParameter\nValues\n\n\n\n\nRelevant Guidelines on Cyber Security\nTo be defined, needs to be researched by Owner.\n\n\nTime Point\nThe end of the project\n\n\n\nPhases: development, testing, validation\nOwner\n\nWP 5 lead\n\nStakeholders\n\nWP 2 (development)\n\nWP 3 (design)\n\nWP 5 (regulatory pathway to market)\n\n\nWish\n\nTool should follow relevant guidelines on cyber security that are within our scope and resource limitations to prepare for compliance with the MDR by the end of the project.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2 Technical Robustness and Safety (ROB)</span>"
    ]
  },
  {
    "objectID": "2_rob.html#fallback-plan-and-general-safety",
    "href": "2_rob.html#fallback-plan-and-general-safety",
    "title": "2 Technical Robustness and Safety (ROB)",
    "section": "2.2 Fallback plan and general safety",
    "text": "2.2 Fallback plan and general safety\nAI systems should have safeguards that enable a fallback plan in case of problems. This can mean that AI systems switch from a statistical to rule-based procedure, or that they ask for a human operator before continuing their action. It must be ensured that the system will do what it is supposed to do without harming living beings or the environment. This includes the minimisation of unintended consequences and errors. In addition, processes to clarify and assess potential risks associated with the use of AI systems, across various application areas, should be established. The level of safety measures required depends on the magnitude of the risk posed by an AI system, which in turn depends on the system’s capabilities. Where it can be foreseen that the development process or the system itself will pose particularly high risks, it is crucial for safety measures to be developed and tested proactively. \n\nRequirement 22-1-ROB\nDescription: A risk assessment following relevant aspects of risk management norms will be done by time point.\n\n\n\n\n\n\n\nParameter\nValues\n\n\n\n\nRisk Management Norms\n- ISO 14971- ISO 23894:2023\n\n\nTime Point\n- When intended purpose is defined- At the end of the project\n\n\n\nPhases: development, testing, validation\nOwner\n\nWP 5 lead\n\nStakeholders\n\nWP 2 (development)\n\nWP 3 (design)\n\nWP 5 (regulatory pathway to market)\n\n\nGoal\n\nA risk assessment following relevant aspects of ISO 14971 will be done by the end of the project.\n\n\n\nWish\n\nA risk assessment following relevant aspects of ISO 23894:2023 will be done when intended purpose is defined.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2 Technical Robustness and Safety (ROB)</span>"
    ]
  },
  {
    "objectID": "2_rob.html#accuracy",
    "href": "2_rob.html#accuracy",
    "title": "2 Technical Robustness and Safety (ROB)",
    "section": "2.3 Accuracy",
    "text": "2.3 Accuracy\nAccuracy pertains to an AI system’s ability to make correct judgements, for example to correctly classify information into the proper categories, or its ability to make correct predictions, recommendations, or decisions based on data or models. An explicit and well-formed development and evaluation process can support, mitigate and correct unintended risks from inaccurate predictions. When occasional inaccurate predictions cannot be avoided, it is important that the system can indicate how likely these errors are. A high level of accuracy is especially crucial in situations where the AI system directly affects human lives. \n\nRequirement 23-1-ROB\nDescription: Validity will be measured by doing a randomized clinical trial (RCT) to examine whether at least ratio of RCT study size patients whose doctors use the tool have a significantly improved Modified Ranking Scale 90 (MRS 90) compared to control group at time point.\n\n\n\n\n\n\n\nParameter\nValues\n\n\n\n\nRatio\nWill be determined at the end of the prospective study\n\n\nRCT Study Size\nNumber will be determined later by estimating the effect size\n\n\nTime Point\nAfter the project\n\n\n\nPhases: validation\nOwner\n\nWP 4 lead\n\nStakeholders\n\nWP 4 (clinical validation)\n\n\nWish\n\nValidity will be measured by doing a randomized clinical trial (RCT) to examine whether at least ratio of number determined by estimated effect size patients whose doctors use the tool have a significantly improved MRS 90 compared to the control group after the project.\n\n\n\n\nRequirement 23-2-ROB\nDescription: Accuracy will be measured by time point by doing a prospective shadowing study to examine whether in at least threshold of the cases, the predictions of the prototype are similar to the actual 3 months mRS according to the actual administered treatment.\n\n\n\n\n\n\n\nParameter\nValues\n\n\n\n\nThreshold\n90%\n\n\nTime Point\nThe end of the project\n\n\n\nPhases: validation\nOwner\n\nWP 4 lead\n\nStakeholders\n\nWP 4 (clinical validation)\n\n\nGoal\n\nAccuracy will be measured by the end of the project by doing a prospective shadowing study to examine whether in at least 90% of the cases, the predictions of the prototype are similar to the actual 3 months mRS according to the actual administered treatment.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2 Technical Robustness and Safety (ROB)</span>"
    ]
  },
  {
    "objectID": "2_rob.html#reliability-and-reproducibility",
    "href": "2_rob.html#reliability-and-reproducibility",
    "title": "2 Technical Robustness and Safety (ROB)",
    "section": "2.4 Reliability and Reproducibility",
    "text": "2.4 Reliability and Reproducibility\nIt is critical that the results of AI systems are reproducible, as well as reliable. A reliable AI system is one that works properly with a range of inputs and in a range of situations. This is needed to scrutinise an AI system and to prevent unintended harms. Reproducibility describes whether an AI experiment exhibits the same behaviour when repeated under the same conditions. This enables scientists and policy makers to accurately describe what AI systems do. Replication files can facilitate the process of testing and reproducing behaviours. \n\nRequirement 24-1-ROB\nDescription: Tool reliably provides no significantly different performance for [contexts] at [time point].\n\n\n\n\n\n\n\nParameter\nValues\n\n\n\n\nContexts\n- Edge-cases- Differing local standards of care- Non-western patient minorities\n\n\nTime Point\nAt the end of the project\n\n\n\nPhases: development, testing, validation\nOwner\n\nWP 2 lead\n\nStakeholders\n\nWP 2 (development)\n\nWP 3 (design)\n\nWP 4 (clinical validation)\n\n\nGoal\n\nTool reliably provides no significantly different performance for edge-cases at the end of the project.\nTool reliably provides no significantly different performance for differing local standards of care at the end of the project.\nTool reliably provides no significantly different performance for non-western patient minorities at the end of the project.\n\n\n\n\nRequirement 24-2-ROB\nDescription: The stability of the accuracies and performance for [different risk areas] of the tool is analyzed or monitored, available by [time point].\n\n\n\n\n\n\n\nParameter\nValues\n\n\n\n\nDifferent Risk Areas\n- Edge-cases- Differing local standards of care- Non-western patient minorities- When there are missing input data points- For sub-groups mentioned in 61-DIV\n\n\nTime Point\nBy the start of the validation phase. See “How the requirements are organized” on page 7.\n\n\n\nPhases: development, testing, validation\nOwner\n\nWP 2 lead\n\nStakeholders\n\nWP 2 (development)\n\nWP 3 (design)\n\nWP 4 (clinical validation)\n\n\nTolerable\n\nThe stability of the accuracies and performance for when there are missing input data points of the tool is analyzed or monitored, available by the start of the validation phase.\n\n\n\nGoal\n\nThe stability of the accuracies and performance for edge-cases, differing local standards of care, non-western patient minorities of the tool is analyzed or monitored, available by the validation phase.\nThe stability of the accuracies and performance for sub-groups mentioned in 61-DIV of the tool is analyzed or monitored, available by the validation phase.\n\n\n\n\nRequirement 24-3-ROB\nDescription: The tool has a defined intended purpose following the MDR [time point].\n\n\n\nParameter\nValues\n\n\n\n\nTime Point\nAs part of the prototype development\n\n\n\nPhases: development, testing\nOwner\n\nWP 3 lead\n\nStakeholders\n\nWP 2 (development)\n\nWP 4 (clinical validation)\n\nWP 5 (regulations)\n\n\nTolerable\n\nThe tool has a defined intended purpose following the MDR as part of the prototype development.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2 Technical Robustness and Safety (ROB)</span>"
    ]
  },
  {
    "objectID": "3_pri.html",
    "href": "3_pri.html",
    "title": "3 Privacy and data governance (PRI)",
    "section": "",
    "text": "3.1 Privacy and data protection\nLike many technologies, AI systems can equally enable and hamper fundamental rights. They can benefit people for instance by helping them track their personal data, or by increasing the accessibility of education, hence supporting their right to education. However, given the reach and capacity of AI systems, they can also negatively affect fundamental rights. In situations where such risks exist, a fundamental rights impact assessment should be undertaken. This should be done prior to the system’s development and include an evaluation of whether those risks can be reduced or justified as necessary in a democratic society in order to respect the rights and freedoms of others. Moreover, mechanisms should be put into place to receive external feedback regarding AI systems that potentially infringe on fundamental rights.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3 Privacy and data governance (PRI)</span>"
    ]
  },
  {
    "objectID": "3_pri.html#privacy-and-data-protection",
    "href": "3_pri.html#privacy-and-data-protection",
    "title": "3 Privacy and data governance (PRI)",
    "section": "",
    "text": "Requirement 31-1-PRI\nDescription: The Prototype complies with relevant [privacy regulations] by [time point].\n\n\n\n\n\n\n\nParameter\nValues\n\n\n\n\nPrivacy Regulations\n- GDPR- Health Insurance Portability and Accountability Act (HIPAA)- National/local authority privacy regulations\n\n\nTime Point\nStart of the prospective study\n\n\n\nPhases: development, testing, validation\nOwner\n\nWP 5 lead\n\nStakeholders\n\nWP 1 (ethics)\n\nWP 2 (development)\n\nWP 3 (design)\n\nWP 5 (regulations)\n\n\nTolerable\n\nThe prototype complies with GDPR by start of prospective study.\nThe prototype complies with Horizon Europe grant requirements for privacy by start of prospective study.\nThe prototype complies with national/local authority privacy regulations by start of prospective study.\n\n\n\nWish\n\nThe prototype complies with HIPAA by start of prospective study.\n\n\n\n\nRequirement 31-2-PRI\nDescription: All [development materials] are stored using [best practices for privacy], with practice enabled before [time point].\n\n\n\n\n\n\n\nParameter\nValues\n\n\n\n\nDevelopment Materials\n- Research data- Models- Predictions- xAI results\n\n\nBest Practices for Privacy\n- Data encryption- Password protected user rights system- Local protected servers on clinical premises\n\n\nTime Point\nStart of trainingAt 24 months into the project\n\n\n\nPhases: development, testing, validation\nOwner\n\nWP 2 lead\n\nStakeholders\n\nWP 2 (development)\n\nWP 3 (design)\n\n\nTolerable\n\nResearch data are stored using password protected user rights system, with practice enabled before start of training.\nResearch data are stored using local protected servers on clinical premises, with practice enabled before start of training.\nModels are stored using password protected user rights system, with practice enabled before start of training.\nModels are stored using local protected servers on clinical premises, with practice enabled before start of training.\nPredictions are stored using password protected user rights system, with practice enabled before start of training.\nPredictions are stored using local protected servers on clinical premises, with practice enabled before start of training.\nxAI results are stored using password protected user rights system, with practice enabled before start of training. xAI results are stored using local protected servers on clinical premises, with practice enabled before start of training.\n\n\n\nGoal\n\nResearch data are stored using data encryption, with practice enabled at 24 months into the project.\nModels are stored using data encryption, with practice enabled at 24 months into the project.\nPredictions are stored using data encryption, with practice enabled at 24 months into the project.\nxAI results are stored using data encryption, with practice enabled at 24 months into the project.\n\n\n\n\nRequirement 31-3-PRI\nDescription: Privacy information have been collected to answer [local ethics committee questions] at [time point] to approve data collection for prospective study in the research data management plan.\n\n\n\n\n\n\n\nParameter\nValues\n\n\n\n\nLocal Ethics Committee Questions\n- Why are we collecting this data?- How will the data be collected?- Where will data be stored?- What data be collected?- Who is the owner of the data?- Who is responsible for the data?- Who has access to the data?- Will the data be transferred to other countries?- Are those countries in the EU? Will the data be shared?\n\n\nTime Point\nBy the end of the development phase\n\n\n\nPhases: development\nOwner\n\nWP 4 lead\n\nStakeholders\n\nWP 2 (development)\n\nWP 3 (design)\n\nWP 5 (regulations)\n\n\nTolerable\n\nPrivacy information have been collected to answer Why are we collecting this data? How will the data be collected? Where will data be stored? What data be collected? Will the data be shared? by the end of the development phase to approve data collection for prospective study in the research data management plan.\n\n\n\n\nRequirement 31-4-PRI\nDescription: Consent from patients have been collected for [data purposes] at [time point].\n\n\n\n\n\n\n\nParameter\nValues\n\n\n\n\nData Purposes\n- Model training using the prospective data- Prospective study- Randomized Clinical Trial- Open data-sharing required by the EU Horizon grant\n\n\nTime Point\n- Before data inclusion in CRF- Before the randomized clinical trial- After the VALIDATE project\n\n\n\nPhases: development, training, validation\nOwner\n\nWP 4 lead\n\nStakeholders\n\nWP 3 (design)\n\nWP 4 (clinical validation)\n\nWP 5 (regulations)\n\nWP 6 (patient communication)\n\n\nTolerable\n\nConsent from patients have been collected for model training using the prospective data before data inclusion in CRF.\nConsent from patients have been collected for open data-sharing required by the EU Horizon grant before data inclusion in CRF.\n\n\n\nGoal\n\nConsent from patients have been collected for prospective study before data inclusion in CRF.\n\n\n\nWish\n\nConsent from patients have been collected for randomized clinical trial before the randomized clinical trial, after the VALIDATE project.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3 Privacy and data governance (PRI)</span>"
    ]
  },
  {
    "objectID": "3_pri.html#quality-and-integrity-of-data",
    "href": "3_pri.html#quality-and-integrity-of-data",
    "title": "3 Privacy and data governance (PRI)",
    "section": "3.2 Quality and integrity of data",
    "text": "3.2 Quality and integrity of data\nThe quality of the data sets used is paramount to the performance of AI systems. When data is gathered, it may contain socially constructed biases, inaccuracies, errors and mistakes. This needs to be addressed prior to training with any given data set. In addition, the integrity of the data must be ensured. Feeding malicious data into an AI system may change its behaviour, particularly with self-learning systems. Processes and data sets used must be tested and documented at each step such as planning, training, testing and deployment. This should also apply to AI systems that were not developed in-house but acquired elsewhere. \n\nRequirement 32-1-PRI\nDescription: % of datasets fulfill [quality criteria] with regards to [quality parameters] at [time point].\n\n\n\n\n\n\n\nParameter\nValues\n\n\n\n\nQuality Criteria\n- Relevant interoperability standards for labeling- HL7 FHIR standards- To be defined\n\n\nQuality Parameters\n- Missing data- Errors- Inaccuracies- Interoperability\n\n\nTime Point\nBefore final model training for prototype\n\n\n\nPhases: development, training, validation\nOwner\n\nWP 2 lead\n\nStakeholders\n\nWP 1 (ethics)\n\nWP 2 (development)\n\nWP 4 (clinical validation)\n\n\nTolerable\n\n100% of datasets fulfill quality criteria: To be defined with regards to missing data, errors, inaccuracies before final model training for prototype.\n\n\n\nGoal\n\n100% of datasets fulfill relevant interoperability standards for labeling with regards to interoperability before final model training for prototype.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3 Privacy and data governance (PRI)</span>"
    ]
  },
  {
    "objectID": "3_pri.html#access-to-data",
    "href": "3_pri.html#access-to-data",
    "title": "3 Privacy and data governance (PRI)",
    "section": "3.3 Access to data",
    "text": "3.3 Access to data\nIn any given organisation that handles individuals’ data (whether someone is a user of the system or not), data protocols governing data access should be put in place. These protocols should outline who can access data and under which circumstances. Only duly qualified personnel with the competence and need to access individual’s data should be allowed to do so. \n\nRequirement 33-1-PRI\nDescription: [Data type] is available for access to [stakeholder type] through [process] at [time point].\n\n\n\n\n\n\n\nParameter\nValues\n\n\n\n\nData Type\n- Retrospective study data- Anonymized prospective study data- Data used to simulate a decision during prospective study- Data generated by the system or app- Meta-data or usage-data of the app/system\n\n\nStakeholder Type\n- Relevant VALIDATE staff- Researchers- Legal guardian/caregiver- Patient- Users\n\n\nProcess\nProcess outlined in VALIDATE data management plan\n\n\nTime Point\n- After embargo period- At the end of the project as preparation for an RCT- For prototype development- During prospective study- At the end of the project\n\n\n\nPhases: development, training, validation\nOwner\n\nWP 3 lead\n\nStakeholders\n\nWP 2 (development)\n\nWP 4 (clinical validation)\n\nWP 5 (admin)\n\n\nTolerable\n\nRetrospective study data is available for access to relevant VALIDATE staff through process outlined in VALIDATE data management plan for prototype development.\nAnonymized prospective study data is available for access to researchers through process outlined in VALIDATE data management plan after embargo period.\nData used to simulate a decision during prospective study is available for access to researchers through process outlined in VALIDATE data management plan during prospective study.\nData generated by the system or app is available for access to users through process outlined in VALIDATE data management plan during prospective study.\n\n\n\nGoal\n\nData used to simulate a decision during prospective study is available for access to patients through process outlined in VALIDATE data management plan during prospective study.\n\n\n\nWish\n\nData used to simulate a decision during prospective study is available for access to legal guardians/caregivers through process outlined in VALIDATE data management plan during prospective study.\nMeta-data or usage-data of the app/system is available for access to researchers through process outlined in VALIDATE data management plan at the end of the project.\n\n\n\n\nRequirement 33-2-PRI\nDescription: Access and use of [sensitive data attributes] is [safeguards] to protect [vulnerable groups] from discrimination and harm by [time point].\n\n\n\n\n\n\n\nParameter\nValues\n\n\n\n\nSensitive Data Attributes\n- Sex- Gender- Ethnicity\n\n\nSafeguards\n- Logged- Only available to relevant qualified personnel with user privileges- Only available after log-in- Data is stored on local hospital premises\n\n\nTime Point\nBy the start of the validation phase. See page 7 of the ethical framework document for the definition of the validation phase.\n\n\n\nPhases: development, training, validation\nOwner\n\nWP 4 lead\n\nStakeholders\n\nWP 2 (development)\n\nWP 4 (clinical validation)\n\nWP 5 (admin)\n\nNORA (data storage)\n\n\nGoal\n\nAccess and use of sex; gender; ethnicity is logged to protect non-binary patients; trans patients; ethnic minorities from discrimination and harm by the start of the validation phase.\nAccess and use of sex; gender; ethnicity is only available to relevant qualified personnel with user privileges to protect non-binary patients; trans patients; ethnic minorities from discrimination and harm by the start of the validation phase.\nAccess and use of sex; gender; ethnicity is only available after log-in to protect non-binary patients; trans patients; ethnic minorities from discrimination and harm by the start of the validation phase.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3 Privacy and data governance (PRI)</span>"
    ]
  },
  {
    "objectID": "4_tra.html",
    "href": "4_tra.html",
    "title": "4 Transparency (TRA)",
    "section": "",
    "text": "4.1 Traceability\nThe data sets and the processes that yield the AI system’s decision, including those of data gathering and data labelling as well as the algorithms used, should be documented to the best possible standard to allow for traceability and an increase in transparency. This also applies to the decisions made by the AI system. This enables identification of the reasons why an AI-decision was erroneous which, in turn, could help prevent future mistakes. Traceability facilitates auditability as well as explainability.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>4 Transparency (TRA)</span>"
    ]
  },
  {
    "objectID": "4_tra.html#traceability",
    "href": "4_tra.html#traceability",
    "title": "4 Transparency (TRA)",
    "section": "",
    "text": "Requirement 41-1-TRA\nDescription: [limitations and metadata] are available to doctors in the app during emergencies by [time point].\n\n\n\n\n\n\n\nParameter\nValues\n\n\n\n\nLimitations and Metadata\n- Data collection trail- Data source- How patient-specific input affected the predictions- How model is calibrated and optimized, especially whether it has a tendency for false negatives or false positives\n\n\nTime Point\nBy the end of the project\n\n\n\nPhases: development, testing, validation\nOwner\n\nWP 2 lead\n\nStakeholders\n\nWP 2 (development)\nWP 3 (design)\n\n\nWish\n\nData collection trail, data source, demographics, how patient-specific input affected the predictions, how model is calibrated and optimized are available to doctors in the app during emergencies by the end of the project.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>4 Transparency (TRA)</span>"
    ]
  },
  {
    "objectID": "4_tra.html#explainability",
    "href": "4_tra.html#explainability",
    "title": "4 Transparency (TRA)",
    "section": "4.2 Explainability",
    "text": "4.2 Explainability\nExplainability concerns the ability to explain both the technical processes of an AI system and the related human decisions (e.g. application areas of a system). Technical explainability requires that the decisions made by an AI system can be understood and traced by human beings. Moreover, trade-offs might have to be made between enhancing a system’s explainability (which may reduce its accuracy) or increasing its accuracy (at the cost of explainability). Whenever an AI system has a significant impact on people’s lives, it should be possible to demand a suitable explanation of the AI system’s decision-making process. Such explanation should be timely and adapted to the expertise of the stakeholder concerned (e.g. layperson, regulator or researcher). In addition, explanations of the degree to which an AI system influences and shapes the organisational decision-making process, design choices of the system, and the rationale for deploying it, should be available (hence ensuring business model transparency).\n\nRequirement 42-1-TRA\nDescription: The system is able to inform about parts of the input that led to a specific outcome, achieved at [time point].\n\n\n\nParameter\nValues\n\n\n\n\nTime Point\nPrior to start of the prospective study\n\n\n\nPhases: development, testing\nOwner\n\nWP 2 lead\n\nStakeholders\n\nWP 2 (development)\nWP 3 (design)\n\n\nTolerable\n\nThe system is able to inform about parts of the input that led to a specific outcome achieved prior to start of the prospective study point.\n\n\n\n\nRequirement 42-2-TRA\nDescription: % of [explanations] are communicated and defined in line with the [constraints] of the project [time point].\n\n\n\n\n\n\n\nParameter\nValues\n\n\n\n\nConstraints\n- Ethical values of the project- Explainability regulations\n\n\nEthical values of the project\nEthical framework\n\n\nExplanations\nHow the AI model works; how the model has been trained; what data has been used; who participated in the study; how the data collected from prospective study is used and shared; how data collected from patients during use will be used?\n\n\nExplainability regulations\nEU AI Act, GDPR, EU-MDR\n\n\nTime Point\nPrior to the start of the prospective study\n\n\n\nPhases: development, testing\nOwner\n\nWP 6 lead\n\nStakeholders\n\nWP 6 (patient communication)\n\n\nGoal\n\n% of explanations are communicated and defined in line with ethical values of the project: Ethical framework of the project prior to start of the prospective study.\n% of explanations are communicated and defined in line with explainability regulations: EU AI act;GDPR;EU-MDR of the project prior to start of the prospective study.\n\n\n\n\nRequirement 42-3-TRA\nDescription: % of [explainability methods] are applied in the tool and outputs are available to [relevant users] by [time point].\n\n\n\n\n\n\n\nParameter\nValues\n\n\n\n\nExplainability methods\n- Explainability methods for tabular data- Explainability methods for imaging data\n\n\nRelevant users\nMedical personnel\n\n\nTime Point\nStart of the prospective study\n\n\n\nPhases: development, testing\nOwner\n\nWP 2 lead\n\nStakeholders\n\nWP 2 (development)\nWP 3 (design)\nWP 4 (clinical validation)\n\n\nGoal\n\n100% of explainability methods for tabular data are applied in the tool and outputs are available to medical personnel by the start of the prospective study.\n100% of explainability methods for imaging data are applied in the tool and outputs are available to medical personnel by the start of the prospective study.\n\n\n\n\nRequirement 42-4-TRA\nDescription: % of [explainability methods] applied in the tool are validated by [metric] by [time point].\n\n\n\n\n\n\n\nParameter\nValues\n\n\n\n\nExplainability methods\n- Explainability methods for tabular and imaging data\n\n\nMetric\n- Quantified scores- Quantified validation by users\n\n\nTime Point\nMonth 24 (delivery date of D2.2 which will incorporate results from T2.4 (refinement and improvement of models for stroke outcome) and T2.5 (augmentation of models with xAI))\n\n\n\nPhases: development, testing\nOwner\n\nWP 2 lead\n\nStakeholders\n\nWP 2 (development)\nWP 3 (design)\n\n\nGoal\n\n100% of explainability methods for tabular data applied in the tool are validated by quantified scores by month 24\n100% of explainability methods for tabular data applied in the tool are validated by qualitative validation by users by month 24\n100% of explainability methods for imaging data applied in the tool are validated by quantified scores by month 24\n100% of explainability methods for imaging data applied in the tool are validated by qualitative validation by users by month 24",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>4 Transparency (TRA)</span>"
    ]
  },
  {
    "objectID": "4_tra.html#communication",
    "href": "4_tra.html#communication",
    "title": "4 Transparency (TRA)",
    "section": "4.3 Communication",
    "text": "4.3 Communication\nAI systems should not represent themselves as humans to users; humans have the right to be informed that they are interacting with an AI system. This entails that AI systems must be identifiable as such. In addition, the option to decide against this interaction in favour of human interaction should be provided where needed to ensure compliance with fundamental rights. Beyond this, the AI system’s capabilities and limitations should be communicated to AI practitioners or end-users in a manner appropriate to the use case at hand. This could encompass communication of the AI system’s level of accuracy, as well as its limitations.\n\nRequirement 43-1-TRA\nDescription: [Information] for [end-users] are explained and [communicated appropriately], as well as translated by a science communicator when relevant, before [time point].\n\n\n\n\n\n\n\n\nParameter\nValues\n\n\n\n\n\n\nPatient, Patient family/caregiver questions\nMedical professional questions\n\n\nInformation\n- Why are we using the tool?&lt;br&gt;- Why don’t we know what the best treatment is?&lt;br&gt;- What are the results of the AI model? &lt;br&gt;- What do the results of the tool imply for them?\n- Why should I use this tool? &lt;br&gt;- When should I not use this tool? &lt;br&gt;- How do we know that this tool is successful? &lt;br&gt;- How does the model work? &lt;br&gt;- Which parameters/variables are input data? And which ones are deciding factors?\n\n\nEnd-users\n- Patients- Patient family/caregivers- Medical professionals\n\n\n\nCommunicated appropriately\nTailored to users in different countries\n\n\n\nTime Point\nStart of the prospective study\n\n\n\n\nPhases: development, testing, validation\nOwner\n\nWP 6 lead\n\nStakeholders\n\nExplainee groups\nWP 2 (development)\nWP 4 (clinical validation)\nWP 6 (patient communication)\n\n\nTolerable\n\nWhy are we using the tool? Why don’t we know what the best treatment is? What are the results of the AI model? What do the results of the tool imply for them? for patients are explained and tailored to users in different countries, as well as translated by a science communicator when relevant, before the start of the prospective study.\nWhy should I use this tool? When should I not use this tool? How do we know that this tool is successful? How does the model work? Which parameters/variables are input data? And which ones are deciding factors? for medical professionals are explained and tailored to users in different countries, as well as translated by a science communicator when relevant, before the start of the prospective study.\n\n\n\nGoal\n\nWhy are we using the tool? Why don’t we know what the best treatment is? What are the results of the AI model? What do the results of the tool imply for them? for patient family/caregivers are explained and tailored to users in different countries, as well as translated by a science communicator when relevant, before the start of the prospective study.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>4 Transparency (TRA)</span>"
    ]
  },
  {
    "objectID": "5_div.html",
    "href": "5_div.html",
    "title": "5 Diversity, non-discrimination, and fairness (DIV)",
    "section": "",
    "text": "5.1 Avoidance of unfair bias\nData sets used by AI systems (both for training and operation) may suffer from the inclusion of inadvertent historic bias, incompleteness and bad governance models. The continuation of such biases could lead to unintended (in)direct prejudice and discrimination against certain groups or people, potentially exacerbating prejudice and marginalisation. Harm can also result from the intentional exploitation of (consumer) biases or by engaging in unfair competition, such as the homogenisation of prices by means of collusion or a non-transparent market. Identifiable and discriminatory bias should be removed in the collection phase where possible. The way in which AI systems are developed (e.g. algorithms’ programming) may also suffer from unfair bias. This could be counteracted by putting in place oversight processes to analyse and address the system’s purpose, constraints, requirements and decisions in a clear and transparent manner. Moreover, hiring from diverse backgrounds, cultures and disciplines can ensure diversity of opinions and should be encouraged.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>5 Diversity, non-discrimination, and fairness (DIV)</span>"
    ]
  },
  {
    "objectID": "5_div.html#avoidance-of-unfair-bias",
    "href": "5_div.html#avoidance-of-unfair-bias",
    "title": "5 Diversity, non-discrimination, and fairness (DIV)",
    "section": "",
    "text": "Requirement 51-1-DIV\nDescription: Subgroup stratified according to [features] AND [intersectional feature combination] has prediction accuracy with no significant difference compared to the majority population.\n\n\n\n\n\n\n\nParameter\nValues\n\n\n\n\nFeatures\nAge, sex/gender, ethnicity/race, geographic location\n\n\nIntersectional feature combination\nSex/gender with ethnicity/race\n\n\n\nPhases: development, testing, validation\nOwner\n\nWP 2 lead\n\nStakeholders\n\nWP 2 (development)\n\n\nGoal\n\nSubgroup stratified according to age has prediction accuracy with no significant difference to the majority population.\nSubgroup stratified according to sex/gender has prediction accuracy with no significant difference to the majority population.\nSubgroup stratified according to ethnicity/race has prediction accuracy with no significant difference to the majority population.\nSubgroup stratified according to geographic location has prediction accuracy with no significant difference to the majority population.\nSubgroup stratified according to sex/gender and ethnicity/race has prediction accuracy with no significant difference to the majority population.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>5 Diversity, non-discrimination, and fairness (DIV)</span>"
    ]
  },
  {
    "objectID": "5_div.html#accessibility-and-universal-design",
    "href": "5_div.html#accessibility-and-universal-design",
    "title": "5 Diversity, non-discrimination, and fairness (DIV)",
    "section": "5.2 Accessibility and Universal Design",
    "text": "5.2 Accessibility and Universal Design\nParticularly in business-to-consumer domains, systems should be user-centric and designed in a way that allows all people to use AI products or services, regardless of their age, gender, abilities or characteristics. Accessibility to this technology for persons with disabilities, which are present in all societal groups, is of particular importance. AI systems should not have a one-size-fits-all approach and should consider Universal Design principles addressing the widest possible range of users, following relevant accessibility standards. This will enable equitable access and active participation of all people in existing and emerging computer-mediated human activities and with regard to assistive technologies.\n\nRequirement 52-1-DIV\nDescription: System undergoes UX/UI testing on X number [user groups] at [time point].\nX number: 6 to 8\n\n\n\n\n\n\n\nParameter\nValues\n\n\n\n\nUser groups\n- Stroke physicians of different technical skill levels- Stroke physicians of different medical skill levels These user groups will later be updated when the User Groups for the Intended Purpose has been defined\n\n\nTime point\nDuring demonstrator development and testing\n\n\n\nPhases: development, testing\nOwner\n\nWP 3 lead\n\nStakeholders\n\nWP 3 (design)\nWP 4 (clinical validation)\n\n\nTolerable\n\nSystem undergoes UX/UI testing on 6-8 stroke physicians of different technical skill level during demonstrator development and testing.\nSystem undergoes UX/UI testing on 6-8 stroke physicians of different medical skill level during demonstrator development and testing.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>5 Diversity, non-discrimination, and fairness (DIV)</span>"
    ]
  },
  {
    "objectID": "5_div.html#stakeholder-participation",
    "href": "5_div.html#stakeholder-participation",
    "title": "5 Diversity, non-discrimination, and fairness (DIV)",
    "section": "5.3 Stakeholder participation",
    "text": "5.3 Stakeholder participation\nIn order to develop AI systems that are trustworthy, it is advisable to consult stakeholders who may directly or indirectly be affected by the system throughout its life cycle. It is beneficial to solicit regular feedback even after deployment and set up longer term mechanisms for stakeholder participation, for example by ensuring workers information, consultation and participation throughout the whole process of implementing AI systems at organisations.\n\nRequirement 53-1-DIV\nDescription: System is presented to X [stakeholders] for feedback [time point].\n\n\n\n\n\n\n\nParameter\nValues\n\n\n\n\nStakeholders\nStroke patients and/or patient representatives from different geographical and cultural backgrounds\n\n\nTime point\nDuring demonstrator development and testing\n\n\n\nPhases: development, testing\nOwner\n\nWP 3 lead\n\nStakeholders\n\nWP 3 (design)\nWP 6 (patient communication)\n\n\nTolerable\n- System is presented to 4 stroke patients and/or patient representatives from different geographical and cultural backgrounds for feedback during demonstrator development and testing.\n\n\nGoal\n- System is presented to 7 stroke patients and/or patient representatives from different geographical and cultural backgrounds for feedback during demonstrator development and testing.\n\n\nWish\n- System is presented to 10 stroke patients and/or patient representatives from different geographical and cultural backgrounds for feedback during demonstrator development and testing.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>5 Diversity, non-discrimination, and fairness (DIV)</span>"
    ]
  },
  {
    "objectID": "6_sus.html",
    "href": "6_sus.html",
    "title": "6 Societal and environmental well-being (SUS)",
    "section": "",
    "text": "6.1 Sustainable and environmentally friendly AI\nAI systems promise to help tackling some of the most pressing societal concerns, yet it must be ensured that this occurs in the most environmentally friendly way possible. The system’s development, deployment and use process, as well as its entire supply chain, should be assessed in this regard, e.g. via a critical examination of the resource usage and energy consumption during training, opting for less harmful choices. Measures securing the environmental friendliness of AI systems’ entire supply chain should be encouraged.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6 Societal and environmental well-being (SUS)</span>"
    ]
  },
  {
    "objectID": "6_sus.html#sustainable-and-environmentally-friendly-ai",
    "href": "6_sus.html#sustainable-and-environmentally-friendly-ai",
    "title": "6 Societal and environmental well-being (SUS)",
    "section": "",
    "text": "Requirement 61-1-SUS\nDescription: % of VALIDATE machine learning engineers follow an SOP to track the environmental impact of model training.\nPhases: development, testing\nOwner\n\nWP 2 lead\n\nStakeholders\n\nWP 1 (ethics)\nWP 2 (development)\n\n\nTolerable\n\n50% of VALIDATE machine learning engineers follow an SOP to track the environmental impact of model training.\n\n\n\nGoal\n\n100% of VALIDATE machine learning engineers follow an SOP to reduce the environmental impact of model training",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6 Societal and environmental well-being (SUS)</span>"
    ]
  },
  {
    "objectID": "6_sus.html#social-impact",
    "href": "6_sus.html#social-impact",
    "title": "6 Societal and environmental well-being (SUS)",
    "section": "6.2 Social impact",
    "text": "6.2 Social impact\nUbiquitous exposure to social AI systems in all areas of our lives (be it in education, work, care or entertainment) may alter our conception of social agency, or impact our social relationships and attachment. While AI systems can be used to enhance social skills, they can equally contribute to their deterioration. This could also affect people’s physical and mental wellbeing. The effects of these systems must therefore be carefully monitored and considered.\nNone of the identified requirements could be mapped to this subcategory.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6 Societal and environmental well-being (SUS)</span>"
    ]
  },
  {
    "objectID": "6_sus.html#society-and-democracy",
    "href": "6_sus.html#society-and-democracy",
    "title": "6 Societal and environmental well-being (SUS)",
    "section": "6.3 Society and democracy",
    "text": "6.3 Society and democracy\nBeyond assessing the impact of an AI system’s development, deployment and use on individuals, this impact should also be assessed from a societal perspective, taking into account its effect on institutions, democracy and society at large. The use of AI systems should be given careful consideration particularly in situations relating to the democratic process, including not only political decision-making but also electoral contexts.\nNone of the identified requirements could be mapped to this subcategory.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6 Societal and environmental well-being (SUS)</span>"
    ]
  },
  {
    "objectID": "7_acc.html",
    "href": "7_acc.html",
    "title": "7 Accountability (ACC)",
    "section": "",
    "text": "7.1 Auditability\nAuditability entails the enablement of the assessment of algorithms, data and design processes. This does not necessarily imply that information about business models and intellectual property related to the AI system must always be openly available. Evaluation by internal and external auditors, and the availability of such evaluation reports, can contribute to the trustworthiness of the technology. In applications affecting fundamental rights, including safety-critical applications, AI systems should be able to be independently audited.\nNone of the identified requirements could be mapped to this subcategory. Auditability, however, is a main aspect of the WP1, with audits being performed throughout the project that will, in a co-creation approach, lead to framework updates via interdisciplinary collaboration.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7 Accountability (ACC)</span>"
    ]
  },
  {
    "objectID": "7_acc.html#minimisation-and-reporting-of-negative-impacts",
    "href": "7_acc.html#minimisation-and-reporting-of-negative-impacts",
    "title": "7 Accountability (ACC)",
    "section": "7.2 Minimisation and reporting of negative impacts",
    "text": "7.2 Minimisation and reporting of negative impacts\nBoth the ability to report on actions or decisions that contribute to a certain system outcome, and to respond to the consequences of such an outcome, must be ensured. Identifying, assessing, reporting and minimising the potential negative impacts of AI systems is especially crucial for those (in)directly affected. Due protection must be available for whistle-blowers, NGOs, trade unions or other entities when reporting legitimate concerns about an AI-based system. The use of impact assessments (e.g. red teaming or forms of Algorithmic Impact Assessment) both prior to and during the development, deployment and use of AI systems can be helpful to minimise negative impact. These assessments must be proportionate to the risk that the AI systems pose.\n\nRequirement 72-1-ACC\nDescription: Algorithmic impact assessment following [relevant method] is performed by [time point].\n\n\n\n\n\n\n\nParameter\nValues\n\n\n\n\nRelevant methods\n- MDR- EU AI Act- To be determined\n\n\nTime Point\nBy the end of the project\n\n\n\nPhases: development, testing, validation\nOwner\n\nWP1 lead\n\nStakeholders\n\nWP 1 (ethics)\n\n\nTolerable\n\nAlgorithmic impact assessment using MDR; EU AI Act, relevant method: To be determined performed by the end of the project.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7 Accountability (ACC)</span>"
    ]
  },
  {
    "objectID": "7_acc.html#trade-offs",
    "href": "7_acc.html#trade-offs",
    "title": "7 Accountability (ACC)",
    "section": "7.3 Trade-offs",
    "text": "7.3 Trade-offs\nWhen implementing the above requirements, tensions may arise between them, which may lead to inevitable trade-offs. Such trade-offs should be addressed in a rational and methodological manner within the state of the art. This entails that relevant interests and values implicated by the AI system should be identified and that, if conflict arises, trade-offs should be explicitly acknowledged and evaluated in terms of their risk to ethical principles, including fundamental rights. In situations in which no ethically acceptable trade-offs can be identified, the development, deployment and use of the AI system should not proceed in that form. Any decision about which trade-off to make should be reasoned and properly documented. The decision-maker must be accountable for the manner in which the appropriate trade-off is being made, and should continually review the appropriateness of the resulting decision to ensure that necessary changes can be made to the system where needed.\n\nRequirement 73-1-ACC\nDescription: Research work produced by the STRATIF-AI consortium must adhere to EU Horizon funding guidelines as well as ethical norms regarding [relevant aspects]. This is implemented by [time point].\n\n\n\n\n\n\n\nParameter\nValues\n\n\n\n\nRelevant methods\n- Data availability- Reproducibility- Open access- Distribution- Confidentiality- Originality\n\n\nTime Point\nThroughout the project\n\n\n\nPhases: development, testing, validation\nOwner\n\nWP1 lead\n\nStakeholders\n\nWP 1 (ethics)\n\n\nGoal\n\nResearch work produced by the VALIDATE consortium must adhere to EU Horizon funding guidelines as well as ethical norms regarding data availability. This is implemented throughout the project.\nResearch work produced by the VALIDATE consortium must adhere to EU Horizon funding guidelines as well as ethical norms regarding reproducibility. This is implemented throughout the project.\nResearch work produced by the VALIDATE consortium must adhere to EU Horizon funding guidelines as well as ethical norms regarding open access. This is implemented throughout the project.\nResearch work produced by the VALIDATE consortium must adhere to EU Horizon funding guidelines as well as ethical norms regarding distribution. This is implemented throughout the project.\nResearch work produced by the VALIDATE consortium must adhere to EU Horizon funding guidelines as well as ethical norms regarding confidentiality. This is implemented throughout the project.\nResearch work produced by the VALIDATE consortium must adhere to EU Horizon funding guidelines as well as ethical norms regarding originality. This is implemented throughout the project.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7 Accountability (ACC)</span>"
    ]
  },
  {
    "objectID": "7_acc.html#redress",
    "href": "7_acc.html#redress",
    "title": "7 Accountability (ACC)",
    "section": "7.4 Redress",
    "text": "7.4 Redress\nWhen unjust adverse impact occurs, accessible mechanisms should be foreseen that ensure adequate redress. Knowing that redress is possible when things go wrong is key to ensure trust. Particular attention should be paid to vulnerable persons or groups.\nNone of the identified requirements could be mapped to this subcategory.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7 Accountability (ACC)</span>"
    ]
  },
  {
    "objectID": "wp_tasks.html",
    "href": "wp_tasks.html",
    "title": "Work Packages",
    "section": "",
    "text": "Work Package 1: Ethics",
    "crumbs": [
      "Work Packages"
    ]
  },
  {
    "objectID": "wp_tasks.html#work-package-1-ethics",
    "href": "wp_tasks.html#work-package-1-ethics",
    "title": "Work Packages",
    "section": "",
    "text": "Human Agency and Oversight\n\nHuman Agency\n\nRequirement 12-2-HUM\n\n\n\nHuman Oversight\n\nRequirement 13-1-HUM\n\n\n\n\nAccountability\n\nMinimisation and reporting of negative impacts\n\nRequirement 72-1-ACC\n\n\n\nTrade-offs\n\nRequirement 73-1-ACC",
    "crumbs": [
      "Work Packages"
    ]
  },
  {
    "objectID": "wp_tasks.html#work-package-2-ml",
    "href": "wp_tasks.html#work-package-2-ml",
    "title": "Work Packages",
    "section": "Work Package 2: ML",
    "text": "Work Package 2: ML\n\nHuman Agency and Oversight\n\nHuman Agency\n\nRequirement 12-1-HUM\n\n\n\n\nTechnical Robustness and Safety\n\nReliability and Reproducibility\n\nRequirement 24-1-ROB  \nRequirement 24-2-ROB\n\n\n\n\nPrivacy and data governance\n\nPrivacy and data protection\n\nRequirement 31-2-PRI\n\n\n\nQuality and integrity of data\n\nRequirement 32-1-PRI\n\n\n\n\nTransparency\n\nTraceability\n\nRequirement 41-1-TRA\n\n\n\nExplainability\n\nRequirement 42-3-TRA\nRequirement 42-4-TRA\n\n\n\n\nDiversity, non-discrimination, and fairness\n\nAvoidance of unfair bias\n\nRequirement 51-1-DIV\n\n\n\n\nSocietal and environmental well-being\n\nSustainable and environmentally friendly AI\n\nRequirement 61-1-SUS",
    "crumbs": [
      "Work Packages"
    ]
  },
  {
    "objectID": "wp_tasks.html#work-package-3-design",
    "href": "wp_tasks.html#work-package-3-design",
    "title": "Work Packages",
    "section": "Work Package 3: Design",
    "text": "Work Package 3: Design\n\nTechnical Robustness and Safety\n\nReliability and Reproducibility\n\nRequirement 24-3-ROB\n\n\n\n\nPrivacy and data governance\n\nAccess to data\n\nRequirement 33-1-PRI\n\n\n\n\nTransparency\n\nExplainability\n\nRequirement 42-1-TRA\n\n\n\n\nDiversity, non-discrimination, and fairness\n\nAccessibility and Universal Design\n\nRequirement 52-1-DIV\n\n\n\nStakeholder participation\n\nRequirement 53-1-DIV",
    "crumbs": [
      "Work Packages"
    ]
  },
  {
    "objectID": "wp_tasks.html#work-package-4-clinical-validation",
    "href": "wp_tasks.html#work-package-4-clinical-validation",
    "title": "Work Packages",
    "section": "Work Package 4: Clinical Validation",
    "text": "Work Package 4: Clinical Validation\n\nTechnical Robustness and Safety\n\nAccuracy\n\nRequirement 23-1-ROB\nRequirement 23-2-ROB\n\n\n\n\nPrivacy and data governance\n\nPrivacy and data protection\n\nRequirement 31-3-PRI\nRequirement 31-4-PRI\n\n\n\nAccess to data\n\nRequirement 33-2-PRI",
    "crumbs": [
      "Work Packages"
    ]
  },
  {
    "objectID": "wp_tasks.html#work-package-5-regulations",
    "href": "wp_tasks.html#work-package-5-regulations",
    "title": "Work Packages",
    "section": "Work Package 5: Regulations",
    "text": "Work Package 5: Regulations\n\nTechnical Robustness and Safety\n\nResilience to Attack and Safety\n\nRequirement 21-1-ROB\nRequirement 21-2-ROB\n\n\n\nFallback plan and general safety\n\nRequirement 22-1-ROB\n\n\n\n\nPrivacy and data governance\n\nPrivacy and data protection\n\nRequirement 31-1-PRI",
    "crumbs": [
      "Work Packages"
    ]
  },
  {
    "objectID": "wp_tasks.html#work-package-6-patient-commmunications",
    "href": "wp_tasks.html#work-package-6-patient-commmunications",
    "title": "Work Packages",
    "section": "Work Package 6: Patient Commmunications",
    "text": "Work Package 6: Patient Commmunications\n\nTransparency\n\nExplainability\n\nRequirement 42-2-TRA\n\n\n\nCommunication\n\nRequirement 43-1-TRA",
    "crumbs": [
      "Work Packages"
    ]
  }
]